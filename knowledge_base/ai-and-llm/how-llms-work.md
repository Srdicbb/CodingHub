# How LLMs Work

> Phase 6.1 | Status: Not Started | Priority: HIGH

## Current Knowledge

- Use AI tools (Claude, LM Studio) but don't understand internals
- Discussed local AI models with friends

## Examples / Notes

### Transformers Architecture
- TODO: Attention mechanism (high level)
- TODO: Why transformers replaced RNNs
- TODO: Self-attention and why it matters

### Key Concepts
- TODO: Tokenization - how text becomes tokens
- TODO: Context windows - what limits AI responses
- TODO: Temperature, top-p, top-k - controlling randomness
- TODO: Why AI hallucinates - it's predicting probable next tokens, not "knowing"

### Model Types & Sizes
- TODO: GPT, Claude, Llama, Mistral - differences
- TODO: Parameters (7B, 13B, 70B) - what they mean
- TODO: Quantization (Q4, Q8, GGUF) - quality vs speed tradeoff
- TODO: Fine-tuned vs base models

## Questions / Confusions

- How does the attention mechanism actually work?
- Why do larger models perform better?
- What determines a model's "intelligence"?
