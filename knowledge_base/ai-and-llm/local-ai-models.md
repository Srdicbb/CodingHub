# Local AI Models (LM Studio / Ollama)

> Phase 6.1 | Status: Exploring | Priority: MEDIUM

## Current Knowledge

- Discussed LM Studio and Hugging Face with friends
- Starting to explore local inference

## Examples / Notes

### LM Studio
- TODO: Setting up LM Studio
- TODO: Downloading models from Hugging Face
- TODO: Model format (GGUF)
- TODO: Running local inference

### Ollama
- TODO: Installation and usage
- TODO: API for local model access
- TODO: Integration with applications

### When Local vs API
- TODO: Privacy/security use cases (sensitive data)
- TODO: Cost considerations (API calls vs hardware)
- TODO: Performance comparison (local vs cloud)
- TODO: Hardware requirements (GPU VRAM, RAM, CPU)

## Questions / Confusions

- What GPU do I need for running local models?
- Which local models are best for coding assistance?
- How to integrate local models into .NET applications?
